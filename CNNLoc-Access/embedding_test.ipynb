{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/aries/miniconda3/envs/CNNLOC/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/aries/miniconda3/envs/CNNLOC/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/aries/miniconda3/envs/CNNLOC/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/aries/miniconda3/envs/CNNLOC/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/aries/miniconda3/envs/CNNLOC/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/aries/miniconda3/envs/CNNLOC/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from encoder_model import EncoderDNN\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch as torch\n",
    "\n",
    "import data_helper_413 as DataHelper\n",
    "import model_413 as Model\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config=tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction=0.9\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "\n",
    "base_dir= os.getcwd()\n",
    "test_csv_path=os.path.join(base_dir,'/home/aries/413/CNNLoc-Access/TestData.csv')\n",
    "valid_csv_path=os.path.join(base_dir,'/home/aries/413/CNNLoc-Access/ValuationData.csv')\n",
    "train_csv_path=os.path.join(base_dir,'/home/aries/413/CNNLoc-Access/TrainingData.csv')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9965\n"
     ]
    }
   ],
   "source": [
    "(train_x, train_y), (valid_x, valid_y),(test_x,test_y) = DataHelper.load_data_all(train_csv_path, valid_csv_path,test_csv_path)\n",
    "(train_x,train_y) = Model.filter_building(train_x,train_y,1)\n",
    "(valid_x, valid_y) = Model.filter_building(valid_x, valid_y, 1)\n",
    "nn_model = Model.NN()\n",
    "nn_model._preprocess(train_x[:2000],train_y[:2000],valid_x[:400],valid_y[:400])\n",
    "nn_model._knn_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    # Initialize a list to store the preprocessed data\n",
    "    preprocessed_data = []\n",
    "\n",
    "    # Iterate over each sample in the batch\n",
    "    for sample in data:\n",
    "        # Find the indices and values of non-zero signal strengths\n",
    "        non_zero_indices = np.nonzero(sample)[0]\n",
    "        non_zero_values = sample[non_zero_indices]\n",
    "\n",
    "        # Sort the indices by signal strength in descending order\n",
    "        sorted_indices = non_zero_indices[np.argsort(-non_zero_values)]\n",
    "        sorted_values = non_zero_values[np.argsort(-non_zero_values)]\n",
    "\n",
    "        # Create a matrix for the sample where each row is a one-hot encoded vector\n",
    "        # multiplied by the corresponding signal strength\n",
    "        signal_strength_matrix = np.zeros((len(sorted_indices), 520))\n",
    "        signal_strength_matrix[np.arange(len(sorted_indices)), sorted_indices] = sorted_values\n",
    "\n",
    "        # Append the signal strength matrix to the preprocessed data\n",
    "        preprocessed_data.append(signal_strength_matrix)\n",
    "\n",
    "    # Determine the maximum number of non-zero values in a sample\n",
    "    M = max(matrix.shape[0] for matrix in preprocessed_data)\n",
    "\n",
    "    # Pad each signal strength matrix to have the same number of rows\n",
    "    for i in range(len(preprocessed_data)):\n",
    "        padding = M - preprocessed_data[i].shape[0]\n",
    "        preprocessed_data[i] = np.pad(preprocessed_data[i], ((0, padding), (0, 0)), 'constant')\n",
    "\n",
    "    # Stack the preprocessed data to form the final output\n",
    "    preprocessed_data = np.stack(preprocessed_data)\n",
    "\n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LocalizationModel(nn.Module):\n",
    "    def __init__(self, num_waps, embedding_dim, output_dim):\n",
    "        super(LocalizationModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_waps, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, signal_strengths):\n",
    "        # x is of shape (batch_size, M), where M is the maximum number of non-zero signal strengths in a sample\n",
    "        # signal_strengths is of shape (batch_size, M), corresponding to the signal strengths of the WAPs in x\n",
    "\n",
    "        # Embed the WAP indices\n",
    "        x = self.embedding(x)  # shape: (batch_size, M, embedding_dim)\n",
    "\n",
    "        # Multiply the embeddings by the signal strengths\n",
    "        x = x * signal_strengths.unsqueeze(-1)  # shape: (batch_size, M, embedding_dim)\n",
    "\n",
    "        # Sum across the signal strength dimension to get a single vector for each sample\n",
    "        x = x.sum(dim=1)  # shape: (batch_size, embedding_dim)\n",
    "\n",
    "        # Pass through the fully connected layer to get the output\n",
    "        out = self.fc(x)  # shape: (batch_size, output_dim)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.2583,  0.8301,  0.6216,  0.3729,  0.5244, -0.1723,  0.1024,  0.3812,\n",
      "         0.0656, -1.7183,  1.3714,  1.2918,  0.7563,  0.1842, -0.3303, -0.7168,\n",
      "        -0.4953,  0.7723,  2.7056, -0.6074, -1.2438,  0.9854,  0.2585, -0.4099,\n",
      "        -0.4263,  1.4122, -0.8190, -2.3250,  1.1550,  0.8452, -1.6340, -0.3426,\n",
      "        -0.7726, -1.7961, -0.1756,  1.1305,  0.3386,  0.7110, -0.4861,  1.0858,\n",
      "         0.0863,  0.7967, -1.0174, -0.6660, -0.1497,  1.2780,  0.2714,  1.0834,\n",
      "        -0.5369,  0.5601])\n",
      "tensor(0.2395)\n"
     ]
    }
   ],
   "source": [
    "model = LocalizationModel(num_waps=520, embedding_dim=50, output_dim=2)\n",
    "\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "# Get the embeddings for WAPs 0 and 1\n",
    "embedding_0 = model.embedding.weight.data[0]\n",
    "embedding_1 = model.embedding.weight.data[1]\n",
    "\n",
    "print(embedding_0)\n",
    "\n",
    "# Compute the cosine similarity\n",
    "similarity = cosine_similarity(embedding_0, embedding_1, dim=0)\n",
    "\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10173535814711922\n",
      "(2000, 48, 520)\n",
      "(2000,)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-df439e5defd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object does not support indexing"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "print(nn_model.normalize_valid_x[0].max())\n",
    "\n",
    "pre_x = preprocess_data(nn_model.normalize_x)\n",
    "pre_valid_x = preprocess_data(nn_model.normalize_valid_x)\n",
    "pre_y = nn_model.floorID_y\n",
    "pre_valid_y = nn_model.floorID_valid_y\n",
    "\n",
    "num_classes = 4\n",
    "\n",
    "print(pre_x.shape)\n",
    "print(pre_y.shape)\n",
    "\n",
    "# Convert the datasets to PyTorch tensors\n",
    "pre_x = torch.tensor(pre_x, dtype=torch.long)\n",
    "pre_valid_x = torch.tensor(pre_valid_x, dtype=torch.long)\n",
    "pre_y = torch.tensor(pre_y, dtype=torch.long)\n",
    "pre_valid_y = torch.tensor(pre_valid_y, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(pre_x, pre_y)\n",
    "valid_dataset = TensorDataset(pre_valid_x, pre_valid_y)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(valid_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-d9cb1b4db3cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Separate the signal strengths from the inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0msignal_strengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape: (batch_size, M)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape: (batch_size, M, 520)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the device\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Create the model\n",
    "model = LocalizationModel(num_waps=520, embedding_dim=50, output_dim=num_classes).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Define the number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Initialize lists to store losses and accuracies\n",
    "train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    train_loss, train_correct, total = 0, 0, 0\n",
    "    model.train()\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Separate the signal strengths from the inputs\n",
    "        signal_strengths = inputs.sum(dim=2)  # shape: (batch_size, M)\n",
    "        inputs = (inputs > 0).long()  # shape: (batch_size, M, 520)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, signal_strengths)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        train_correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "    train_accs.append(train_correct / total)\n",
    "\n",
    "    # Validation\n",
    "    val_loss, val_correct, total = 0, 0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, targets) in enumerate(val_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Separate the signal strengths from the inputs\n",
    "            signal_strengths = inputs.sum(dim=2)  # shape: (batch_size, M)\n",
    "            inputs = (inputs > 0).long()  # shape: (batch_size, M, 520)\n",
    "\n",
    "            outputs = model(inputs, signal_strengths)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            val_correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    val_accs.append(val_correct / total)\n",
    "\n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accs[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accs[-1]:.4f}')\n",
    "    \n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train')\n",
    "plt.plot(val_losses, label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accs, label='Train')\n",
    "plt.plot(val_accs, label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CNNLOC-kernal",
   "language": "python",
   "name": "cnnloc-kernal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
